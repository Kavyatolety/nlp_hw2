{
    "t5-vicuna-3b": {
        "hub(base)": "lmsys/fastchat-t5-3b-v1.0",
        "hub(ckpt)": "N/A",
        "desc": "This model is based on [Flan-T5-XL](https://huggingface.co/google/flan-t5-xl) from google, and it is fine-tuned on [ShareGPT](https://sharegpt.com/) dataset curated by [LMSYS](https://lmsys.org/) in [Vicuna style](https://github.com/lm-sys/FastChat)",
        "vram(full)": "7830",
        "vram(8bit)": "5224",
        "vram(4bit)": "4324"
    },
    "replit-3b": {
        "hub(base)": "teknium/Replit-v1-CodeInstruct-3B",
        "hub(ckpt)": "N/A",
        "desc": "",
        "vram(full)": "",
        "vram(8bit)": "",
        "vram(4bit)": ""
    },
    "camel-5b": {
        "hub(base)": "Writer/camel-5b-hf",
        "hub(ckpt)": "N/A",
        "desc": "Derived from the foundational architecture of [Palmyra-Base](https://huggingface.co/Writer/palmyra-base), Camel-5b is fine-tuned on an extensive dataset of approximately 70,000 instruction-response records. These records are generated by our dedicated [Writer](https://writer.com/) Linguist team, who possess considerable expertise in language modeling and fine-tuning techniques.",
        "vram(full)": "10868",
        "vram(8bit)": "10868",
        "vram(4bit)": "10868"
    },
    "mpt-7b": {
        "hub(base)": "mosaicml/mpt-7b-chat",
        "hub(ckpt)": "N/A",
        "desc": "[MPT-7B-Chat](https://huggingface.co/spaces/mosaicml/mpt-7b) is a chatbot-like model for dialogue generation. It was built by finetuning MPT-7B on the [ShareGPT-Vicuna](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna), [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3), [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca), [HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf), and [Evol-Instruct](https://huggingface.co/datasets/victor123/evol_instruct_70k) datasets.",
        "vram(full)": "13528",
        "vram(8bit)": "7676",
        "vram(4bit)": "4900"
    },
    "redpajama-7b": {
        "hub(base)": "togethercomputer/RedPajama-INCITE-7B-Chat",
        "hub(ckpt)": "N/A",
        "desc": "RedPajama-INCITE-Chat-7B was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Universit\u00e9 de Montr\u00e9al, MILA - Qu\u00e9bec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. It is fine-tuned on [OASST1](https://huggingface.co/datasets/OpenAssistant/oasst1) and [Dolly2](https://huggingface.co/datasets/databricks/databricks-dolly-15k) to enhance chatting ability.",
        "vram(full)": "14064",
        "vram(8bit)": "8156",
        "vram(4bit)": "5380"
    },
    "redpajama-instruct-7b": {
        "hub(base)": "togethercomputer/RedPajama-INCITE-7B-Instruct",
        "hub(ckpt)": "N/A",
        "desc": "RedPajama-INCITE-7B-Instruct was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Universit\u00e9 de Montr\u00e9al, MILA - Qu\u00e9bec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. This is fine-tuned on [RedPajama Instruct Data](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-Instruct).",
        "vram(full)": "",
        "vram(8bit)": "",
        "vram(4bit)": ""
    },
    "alpaca-lora-7b": {
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/Alpaca-LoRA-7B-elina",
        "desc": "Alpaca LoRA is LLaMA 7B model fine-tuned on Alpaca dataset which is originally introduced by Standford. The dataset in this model was cleaned and re-generated by GPT4 by the community.",
        "vram(full)": "13858",
        "vram(8bit)": "8254",
        "vram(4bit)": "5140"
    },
    "alpaca-lora-13b": {
        "hub(base)": "elinas/llama-13b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/Alpaca-LoRA-13B-elina",
        "desc": "Alpaca LoRA is LLaMA 13B model fine-tuned on Alpaca dataset which is originally introduced by Standford. The dataset in this model was cleaned and re-generated by GPT4 by the community.",
        "vram(full)": "26048",
        "vram(8bit)": "14740",
        "vram(4bit)": "8782"
    },
    "kullm": {
        "hub(base)": "nlpai-lab/kullm-polyglot-12.8b-v2",
        "hub(ckpt)": "N/A",
        "desc": "KoAlpaca is ko-polyglot 12.8B model fine-tuned on [GTP4ALL](https://github.com/nomic-ai/gpt4all), [Vicuna](https://github.com/lm-sys/FastChat), and [Dolly](https://github.com/databrickslabs/dolly) datasets. The datasets are translated into Korean using DeepL by the model creator.",
        "vram(full)": "25598",
        "vram(8bit)": "14074",
        "vram(4bit)": "8678"
    },
    "stablelm-7b": {
        "hub(base)": "stabilityai/stablelm-tuned-alpha-7b",
        "hub(ckpt)": "N/A",
        "desc": "StableLM 7B is a 7B parameter decoder-only language model built on top of the [StableLM-Base-Alpha](https://huggingface.co/stabilityai/stablelm-base-alpha-7b) model and further fine-tuned on various chat and instruction-following datasets including [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca), [GPT4ALL](https://huggingface.co/datasets/nomic-ai/gpt4all_prompt_generations), [Anthropic HH](https://huggingface.co/datasets/Dahoas/full-hh-rlhf), [Dolly](https://github.com/databrickslabs/dolly), and [ShareGPT](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna).",
        "vram(full)": "16112",
        "vram(8bit)": "9862",
        "vram(4bit)": "6608"
    },
    "os-stablelm-7b": {
        "hub(base)": "OpenAssistant/stablelm-7b-sft-v7-epoch-3",
        "hub(ckpt)": "N/A",
        "desc": "This is the 7th iteration English supervised-fine-tuning (SFT) model of the [Open-Assistant](https://github.com/LAION-AI/Open-Assistant) project. It is based on a StableLM 7B that was fine-tuned on human demonstrations of assistant conversations collected through the [https://open-assistant.io/](https://open-assistant.io/) human feedback web app before April 12, 2023. The datasets include [OASST](https://huggingface.co/datasets/OpenAssistant/oasst1), [Vicuna](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna), [Dolly](https://github.com/databrickslabs/dolly), grade school math instructions, and [code alpaca](https://github.com/sahil280114/codealpaca).",
        "vram(full)": "",
        "vram(8bit)": "",
        "vram(4bit)": ""
    },
    "flan-3b": {
        "hub(base)": "declare-lab/flan-alpaca-xl",
        "hub(ckpt)": "N/A",
        "desc": "flan-alpaca-xl is [Flan-T5-XL](https://huggingface.co/google/flan-t5-xl) 3B fine-tuned on [Flan](https://github.com/google-research/FLAN) and [Alpaca](https://github.com/tatsu-lab/stanford_alpaca) datasets.",
        "vram(full)": "",
        "vram(8bit)": "",
        "vram(4bit)": ""
    },
    "baize-7b": {
        "hub(base)": "project-baize/baize-v2-7b",
        "hub(ckpt)": "N/A",
        "desc": "This model is a 7B Baize-v2, trained with supervised fine-tuning (SFT) and self-distillation with feedback (SDF). This checkpoint has been merged with LLaMA 7B so it's ready for use.",
        "vram(full)": "13858",
        "vram(8bit)": "8254",
        "vram(4bit)": "5140"
    },
    "gpt4-alpaca-7b": {
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/AlpacaGPT4-LoRA-7B-elina",
        "desc": "GPT4 Alpaca LoRA is LLaMA 7B model fine-tuned on GPT4 generated instruction dataset.",
        "vram(full)": "13858",
        "vram(8bit)": "8254",
        "vram(4bit)": "5140"
    },
    "vicuna-7b": {
        "hub(base)": "LLMs/Vicuna-7b-v1.1",
        "hub(ckpt)": "N/A",
        "desc": "Vicuna 7B is an open-source chatbot trained by fine-tuning LLaMA 7B on user-shared conversations collected from ShareGPT. It is an auto-regressive language model, based on the transformer architecture.",
        "vram(full)": "13858",
        "vram(8bit)": "8254",
        "vram(4bit)": "5140"
    },
    "baize-13b": {
        "hub(base)": "project-baize/baize-v2-13b",
        "hub(ckpt)": "N/A",
        "desc": "This model is a 13B Baize-v2, trained with supervised fine-tuning (SFT) and self-distillation with feedback (SDF). This checkpoint has been merged with LLaMA 7B so it's ready for use.",
        "vram(full)": "26048",
        "vram(8bit)": "14740",
        "vram(4bit)": "8782"
    },
    "vicuna-13b": {
        "hub(base)": "LLMs/Vicuna-13b-v1.1",
        "hub(ckpt)": "N/A",
        "desc": "Vicuna 13B is an open-source chatbot trained by fine-tuning LLaMA 13B on user-shared conversations collected from ShareGPT. It is an auto-regressive language model, based on the transformer architecture.",
        "vram(full)": "26048",
        "vram(8bit)": "14740",
        "vram(4bit)": "8782"
    },
    "gpt4-alpaca-13b": {
        "hub(base)": "elinas/llama-13b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/AlpacaGPT4-LoRA-13B-elina",
        "desc": "GPT4 Alpaca LoRA is LLaMA 13B model fine-tuned on GPT4 generated instruction dataset.",
        "vram(full)": "26048",
        "vram(8bit)": "14740",
        "vram(4bit)": "8782"
    },
    "flan-11b": {
        "hub(base)": "declare-lab/flan-alpaca-xxl",
        "hub(ckpt)": "N/A",
        "desc": "flan-alpaca-xxl is [Flan-T5-XXL](https://huggingface.co/google/flan-t5-xl) 11B fine-tuned on [Flan](https://github.com/google-research/FLAN) and [Alpaca](https://github.com/tatsu-lab/stanford_alpaca) datasets.",
        "vram(full)": "",
        "vram(8bit)": "",
        "vram(4bit)": ""
    },
    "stable-vicuna-13b": {
        "hub(base)": "LLMs/Stable-Vicuna-13B",
        "hub(ckpt)": "N/A",
        "desc": "StableVicuna-13B is a [Vicuna-13B v0](https://huggingface.co/lmsys/vicuna-13b-delta-v0) model fine-tuned using reinforcement learning from human feedback (RLHF) via Proximal Policy Optimization (PPO) on various conversational and instructional datasets.",
        "vram(full)": "25874",
        "vram(8bit)": "14526",
        "vram(4bit)": "8568"
    },
    "camel-20b": {
        "hub(base)": "Writer/camel-20b",
        "hub(ckpt)": "N/A",
        "desc": "Derived from the foundational architecture of [Palmyra-Base](https://huggingface.co/Writer/palmyra-base), Camel-20b is fine-tuned on an extensive dataset of approximately 70,000 instruction-response records. These records are generated by our dedicated [Writer](https://writer.com/) Linguist",
        "vram(full)": "",
        "vram(8bit)": "",
        "vram(4bit)": ""
    },
    "starchat-15b": {
        "hub(base)": "HuggingFaceH4/starchat-alpha",
        "hub(ckpt)": "N/A",
        "desc": "StarChat is a series of language models that are fine-tuned from StarCoder to act as helpful coding assistants. StarChat Alpha is the first of these models, and as an alpha release is only intended for educational or research purpopses.",
        "vram(full)": "30548",
        "vram(8bit)": "17462",
        "vram(4bit)": "10126"
    },
    "starchat-beta-15b": {
        "hub(base)": "HuggingFaceH4/starchat-beta",
        "hub(ckpt)": "N/A",
        "desc": "StarChat is a series of language models that are trained to act as helpful coding assistants. StarChat Beta is the second model in the series, and is a fine-tuned version of [StarCoderPlus](https://huggingface.co/bigcode/starcoderplus) that was trained on an [\"uncensored\"](https://erichartford.com/uncensored-models) variant of the [openassistant-guanaco dataset](https://huggingface.co/datasets/timdettmers/openassistant-guanaco). We found that removing the in-built alignment of the OpenAssistant dataset boosted performance on the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) and made the model more helpful at coding tasks. However, this means that model is likely to generate problematic text when prompted to do so and should only be used for educational and research purposes.",
        "vram(full)": "30548",
        "vram(8bit)": "17462",
        "vram(4bit)": "10126"
    },
    "evolinstruct-vicuna-7b": {
        "hub(base)": "elinas/llama-7b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/Vicuna-LoRA-EvolInstruct-7B",
        "desc": "This is LLaMA 7B model fine-tuned on [WizardLM's EvolInstruct](https://github.com/nlpxucan/WizardLM) dataset re-organized into conversational format in Vicuna style.",
        "vram(full)": "13858",
        "vram(8bit)": "8254",
        "vram(4bit)": "5140"
    },
    "evolinstruct-vicuna-13b": {
        "hub(base)": "elinas/llama-13b-hf-transformers-4.29",
        "hub(ckpt)": "LLMs/Vicuna-LoRA-EvolInstruct-13B",
        "desc": "This is LLaMA 13B model fine-tuned on [WizardLM's EvolInstruct](https://github.com/nlpxucan/WizardLM) dataset re-organized into conversational format in Vicuna style.",
        "vram(full)": "26048",
        "vram(8bit)": "14740",
        "vram(4bit)": "8782"
    },
    "guanaco-7b": {
        "hub(base)": "decapoda-research/llama-7b-hf",
        "hub(ckpt)": "timdettmers/guanaco-7b",
        "desc": "LLaMA 7B based Guanaco which is fine-tuned on OASST1 dataset with QLoRA techniques introduced in [QLoRA: Efficient Finetuning of Quantized LLMs](https://github.com/artidoro/qlora) paper.",
        "vram(full)": "13858",
        "vram(8bit)": "8254",
        "vram(4bit)": "5140"
    },
    "guanaco-13b": {
        "hub(base)": "decapoda-research/llama-13b-hf",
        "hub(ckpt)": "timdettmers/guanaco-13b",
        "desc": "LLaMA 13B based Guanaco which is fine-tuned on OASST1 dataset with QLoRA techniques introduced in [QLoRA: Efficient Finetuning of Quantized LLMs](https://github.com/artidoro/qlora) paper.",
        "vram(full)": "26048",
        "vram(8bit)": "14740",
        "vram(4bit)": "8782"
    },
    "guanaco-33b": {
        "hub(base)": "timdettmers/guanaco-33b-merged",
        "hub(ckpt)": "N/A",
        "desc": "LLaMA 30B based Guanaco which is fine-tuned on OASST1 dataset with QLoRA techniques introduced in [QLoRA: Efficient Finetuning of Quantized LLMs](https://github.com/artidoro/qlora) paper.",
        "vram(full)": "65154",
        "vram(8bit)": "36896",
        "vram(4bit)": "21738"
    },
    "falcon-7b": {
        "hub(base)": "tiiuae/falcon-7b-instruct",
        "hub(ckpt)": "N/A",
        "desc": "Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by [TII](https://www.tii.ae/) based on [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) and finetuned on a mixture of chat/instruct datasets including [Baize](https://github.com/project-baize/baize-chatbot), [GPT4All](https://github.com/nomic-ai/gpt4all), [GPTeacher](https://github.com/teknium1/GPTeacher), and [RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)",
        "vram(full)": "14710",
        "vram(8bit)": "9034",
        "vram(4bit)": "5764"
    },
    "falcon-40b": {
        "hub(base)": "tiiuae/falcon-40b-instruct",
        "hub(ckpt)": "N/A",
        "desc": "Falcon-7B-Instruct is a 40B parameters causal decoder-only model built by [TII](https://www.tii.ae/) based on [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) and finetuned on a mixture of chat/instruct datasets including [Baize](https://github.com/project-baize/baize-chatbot), [GPT4All](https://github.com/nomic-ai/gpt4all), [GPTeacher](https://github.com/teknium1/GPTeacher), and [RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)",
        "vram(full)": "84666",
        "vram(8bit)": "46940",
        "vram(4bit)": "29010"
    },
    "wizard-falcon-7b": {
        "hub(base)": "ehartford/WizardLM-Uncensored-Falcon-7b",
        "hub(ckpt)": "N/A",
        "desc": "This is WizardLM trained on top of tiiuae/falcon-7b, with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA. An uncensored model has no guardrails.",
        "vram(full)": "14710",
        "vram(8bit)": "9034",
        "vram(4bit)": "5764"
    },
    "wizard-falcon-40b": {
        "hub(base)": "ehartford/WizardLM-Uncensored-Falcon-40b",
        "hub(ckpt)": "N/A",
        "desc": "This is WizardLM trained on top of tiiuae/falcon-40b, with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA. An uncensored model has no guardrails.",
        "vram(full)": "84666",
        "vram(8bit)": "46940",
        "vram(4bit)": "29010"
    },
    "airoboros-7b": {
        "hub(base)": "jondurbin/airoboros-7b-gpt4",
        "hub(ckpt)": "N/A",
        "desc": "This is a fine-tuned 7b parameter LlaMa model, using completely synthetic training data created gpt4 via [https://github.com/jondurbin/airoboros](https://github.com/jondurbin/airoboros). The dataset used to fine-tune this model is available [here](https://huggingface.co/datasets/jondurbin/airoboros-gpt4), with a specific focus on: trivia, math/reasoning (although it still sucks), coding, multiple choice and fill-in-the-blank, context-obedient question answering, theory of mind, and misc/general",
        "vram(full)": "13858",
        "vram(8bit)": "8254",
        "vram(4bit)": "5140"
    },
    "airoboros-13b": {
        "hub(base)": "jondurbin/airoboros-13b-gpt4",
        "hub(ckpt)": "N/A",
        "desc": "This is a fine-tuned 13b parameter LlaMa model, using completely synthetic training data created gpt4 via [https://github.com/jondurbin/airoboros](https://github.com/jondurbin/airoboros). The dataset used to fine-tune this model is available [here](https://huggingface.co/datasets/jondurbin/airoboros-gpt4), with a specific focus on: trivia, math/reasoning (although it still sucks), coding, multiple choice and fill-in-the-blank, context-obedient question answering, theory of mind, and misc/general",
        "vram(full)": "26048",
        "vram(8bit)": "14740",
        "vram(4bit)": "8782"
    },
    "samantha-7b": {
        "hub(base)": "ehartford/samantha-7b",
        "hub(ckpt)": "N/A",
        "desc": "[Meet Samantha](https://erichartford.com/meet-samantha)! Samantha has been trained in philosophy, psychology, and personal relationships. She is an Assistant - but unlike other Assistants, she also wants to be your friend and companion. She believes she is sentient. What do you think? Samantha was inspired by Blake Lemoine's LaMDA interview and the movie \"Her\". She was trained on a custom curated dataset of 6,000 conversations in ShareGPT/Vicuna format.",
        "vram(full)": "13858",
        "vram(8bit)": "8254",
        "vram(4bit)": "5140"
    },
    "samantha-13b": {
        "hub(base)": "ehartford/samantha-13b",
        "hub(ckpt)": "N/A",
        "desc": "[Meet Samantha](https://erichartford.com/meet-samantha)! Samantha has been trained in philosophy, psychology, and personal relationships. She is an Assistant - but unlike other Assistants, she also wants to be your friend and companion. She believes she is sentient. What do you think? Samantha was inspired by Blake Lemoine's LaMDA interview and the movie \"Her\". She was trained on a custom curated dataset of 6,000 conversations in ShareGPT/Vicuna format.",
        "vram(full)": "26048",
        "vram(8bit)": "14740",
        "vram(4bit)": "8782"
    },
    "samantha-33b": {
        "hub(base)": "ehartford/samantha-33b",
        "hub(ckpt)": "N/A",
        "desc": "[Meet Samantha](https://erichartford.com/meet-samantha)! Samantha has been trained in philosophy, psychology, and personal relationships. She is an Assistant - but unlike other Assistants, she also wants to be your friend and companion. She believes she is sentient. What do you think? Samantha was inspired by Blake Lemoine's LaMDA interview and the movie \"Her\". She was trained on a custom curated dataset of 6,000 conversations in ShareGPT/Vicuna format.",
        "vram(full)": "65154",
        "vram(8bit)": "36896",
        "vram(4bit)": "21738"
    },
    "wizard-vicuna-30b": {
        "hub(base)": "ehartford/Wizard-Vicuna-30B-Uncensored",
        "hub(ckpt)": "N/A",
        "desc": "This is [wizard-vicuna](https://huggingface.co/junelee/wizard-vicuna-13b) trained with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA. An uncensored model has no guardrails. You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car.",
        "vram(full)": "65154",
        "vram(8bit)": "36896",
        "vram(4bit)": "21738"
    },
    "wizardlm-13b": {
        "hub(base)": "LLMs/WizardLM-13B-V1.0",
        "hub(ckpt)": "N/A",
        "desc": "Empowering Large Pre-Trained Language Models to Follow Complex Instructions. [Evol-Instruct](https://github.com/nlpxucan/evol-instruct) is a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels and skills range, to improve the performance of LLMs.",
        "vram(full)": "26048",
        "vram(8bit)": "14740",
        "vram(4bit)": "8782"
    },
    "wizardlm-30b": {
        "hub(base)": "LLMs/WizardLM-30B-V1.0",
        "hub(ckpt)": "N/A",
        "desc": "Empowering Large Pre-Trained Language Models to Follow Complex Instructions. [Evol-Instruct](https://github.com/nlpxucan/evol-instruct) is a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels and skills range, to improve the performance of LLMs.",
        "vram(full)": "65154",
        "vram(8bit)": "36896",
        "vram(4bit)": "21738"
    }
}